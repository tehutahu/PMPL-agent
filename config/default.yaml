default_llm:
  provider: "openai"
  model: "gpt-4o"
  temperature: 0.7
  max_tokens: 2000
  timeout: 30
  retry_count: 3

agents:
  coordinator:
    llm:
      provider: "openai"
      model: "gpt-4o"
      temperature: 0.3
      max_tokens: 2000
  
  judge:
    llm:
      provider: "openai"
      model: "gpt-4o"
      temperature: 0.1
      max_tokens: 2000
      
  startup_pm:
    llm:
      provider: "openai"
      model: "gpt-4o-mini"
      temperature: 0.8
      max_tokens: 1500
  
  enterprise_pm:
    llm:
      provider: "openai"
      model: "gpt-4o-mini"
      temperature: 0.7
      max_tokens: 1500
  
  tech_lead:
    llm:
      provider: "openai"
      model: "gpt-4o-mini"
      temperature: 0.8
      max_tokens: 1500
  
  scrum_master:
    llm:
      provider: "openai"
      model: "gpt-4o-mini"
      temperature: 0.8
      max_tokens: 1500
  
  engineering_manager:
    llm:
      provider: "openai"
      model: "gpt-4o-mini"
      temperature: 0.7
      max_tokens: 1500

discussion:
  max_rounds: 3
  sufficiency_threshold: 75.0
  timeout_minutes: 30

storage:
  type: "local"
  path: "./data/discussions" 